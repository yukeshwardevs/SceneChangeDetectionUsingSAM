{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-08-19T13:49:41.777966Z","iopub.status.busy":"2024-08-19T13:49:41.777065Z","iopub.status.idle":"2024-08-19T13:50:48.648812Z","shell.execute_reply":"2024-08-19T13:50:48.647587Z","shell.execute_reply.started":"2024-08-19T13:49:41.777932Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2)\n","Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.16.2)\n","Requirement already satisfied: opencv-python in /opt/conda/lib/python3.10/site-packages (4.10.0.84)\n","Requirement already satisfied: segment-anything in /opt/conda/lib/python3.10/site-packages (1.0)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.13.1)\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.9.0)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.0)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.5.0)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.26.4)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision) (2.32.3)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (9.5.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2024.7.4)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n","Requirement already satisfied: scikit-image in /opt/conda/lib/python3.10/site-packages (0.22.0)\n","Requirement already satisfied: numpy>=1.22 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (1.26.4)\n","Requirement already satisfied: scipy>=1.8 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (1.11.4)\n","Requirement already satisfied: networkx>=2.8 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (3.2.1)\n","Requirement already satisfied: pillow>=9.0.1 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (9.5.0)\n","Requirement already satisfied: imageio>=2.27 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (2.33.1)\n","Requirement already satisfied: tifffile>=2022.8.12 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (2023.12.9)\n","Requirement already satisfied: packaging>=21 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (21.3)\n","Requirement already satisfied: lazy_loader>=0.3 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (0.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=21->scikit-image) (3.1.1)\n","Collecting git+https://github.com/facebookresearch/segment-anything.git\n","  Cloning https://github.com/facebookresearch/segment-anything.git to /tmp/pip-req-build-_nz7_9fk\n","  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/segment-anything.git /tmp/pip-req-build-_nz7_9fk\n","  Resolved https://github.com/facebookresearch/segment-anything.git to commit 6fdee8f2727f4506cfbbe553e23b895e27956588\n","  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\n","jupyterlab 4.2.3 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\n","jupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\n","tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.4.1 which is incompatible.\n","ydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!pip install torch torchvision opencv-python segment-anything\n","!pip install scikit-image\n","! pip install \\\n","'git+https://github.com/facebookresearch/segment-anything.git'\n","! pip install -q roboflow supervision\n","! wget -q \\\n","'https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth'"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-08-19T13:51:25.496292Z","iopub.status.busy":"2024-08-19T13:51:25.495830Z","iopub.status.idle":"2024-08-19T13:51:34.965952Z","shell.execute_reply":"2024-08-19T13:51:34.964932Z","shell.execute_reply.started":"2024-08-19T13:51:25.496251Z"},"trusted":true},"outputs":[],"source":["import torch\n","from segment_anything import sam_model_registry,SamPredictor\n","\n","DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","MODEL_TYPE = \"vit_h\"\n","\n","sam = sam_model_registry[MODEL_TYPE](checkpoint=\"/kaggle/working/sam_vit_h_4b8939.pth\")\n","sam.to(device=DEVICE)\n","predictor = SamPredictor(sam)\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-08-19T13:51:46.011011Z","iopub.status.busy":"2024-08-19T13:51:46.010098Z","iopub.status.idle":"2024-08-19T13:51:58.813214Z","shell.execute_reply":"2024-08-19T13:51:58.812004Z","shell.execute_reply.started":"2024-08-19T13:51:46.010976Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting youtube-transcript-api\n","  Downloading youtube_transcript_api-0.6.2-py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from youtube-transcript-api) (2.32.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->youtube-transcript-api) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->youtube-transcript-api) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->youtube-transcript-api) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->youtube-transcript-api) (2024.7.4)\n","Downloading youtube_transcript_api-0.6.2-py3-none-any.whl (24 kB)\n","Installing collected packages: youtube-transcript-api\n","Successfully installed youtube-transcript-api-0.6.2\n"]}],"source":["! pip install youtube-transcript-api\n","from youtube_transcript_api import YouTubeTranscriptApi"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-08-19T09:41:44.958694Z","iopub.status.busy":"2024-08-19T09:41:44.957981Z","iopub.status.idle":"2024-08-19T09:45:57.043226Z","shell.execute_reply":"2024-08-19T09:45:57.042284Z","shell.execute_reply.started":"2024-08-19T09:41:44.958647Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Scene changes detected at timestamps (in seconds): [12.762749999999999, 48.21483333333333, 69.48608333333333, 82.24883333333332, 102.10199999999999, 126.20941666666666, 143.22641666666667, 161.6615, 163.07958333333332, 181.51466666666664, 182.93275, 184.35083333333333, 201.36783333333332, 225.47525, 232.56566666666666, 236.81991666666664, 238.23799999999997, 242.49224999999998, 252.4188333333333, 258.09116666666665, 296.37941666666666]\n","Scene 1: I hope I'm able to stand\n","out in this interview. What if I say the wrong thing? Wow, they both look\n","really professional. Am I dressed correctly? [MUSIC PLAYING] If a job you've applied to\n","has a lot of other applicants,\n","\n","Scene 2: chances are you might find\n","yourself starting the interview process in a group setting. But don't be alarmed. Group interviews are a\n","great way for employers to get an initial read on\n","you and your work ethic. And above all, they want\n","to see how you communicate and how you work with a team. While they might seem\n","daunting at first, they don't have to be, if you\n","go in with the right mindset. Today, we're going to highlight\n","some best practices for a group interview. With them in mind, securing\n","a one-on-one interview after the group interview\n","should be no problem. [MUSIC PLAYING]\n","\n","Scene 3: Good morning, everyone. I'm Anne. Good morning. Good to meet you all. Michael. Nice to meet you. Chloe. Nice to meet you. Oh, Chloe, Michael. What's great here is\n","that all three candidates are early to the interview. That's a must. But notice how the\n","first two weren't speaking when Anne walked in. Avoid the urge to\n","check your phone\n","\n","Scene 4: or sit silently while waiting\n","for the interviewer to arrive. Like Anne did, start the\n","conversation right away and get to know the\n","other candidates. This will only help during\n","the actual interview.\n","\n","Scene 5: Speaking of, the\n","interviewer has now arrived. So let's get into it. Chloe, how would you\n","describe yourself? Let's see, I've worked\n","in customer service for the last six years, starting\n","in the restaurant industry and then eventually\n","making my way to retail. When asked during a group\n","interview about yourself,\n","\n","Scene 6: don't just recite what can\n","be found on your resume. Share a personal story,\n","something that the interviewer will remember. I am extremely passionate\n","about working with people. I was named employee\n","of the month two times in my current role because\n","of my problem-solving nature in those situations. And I'm excited to hopefully\n","do that soon here as well. That was a great finish.\n","\n","Scene 7: Don't be afraid to brag a\n","little during a group interview when asked about yourself. Be confident, and bring\n","up your accomplishments, especially if it pertains to\n","the job you're applying for. This will help differentiate\n","you from the other candidates. Let's continue. Michael, tell me about a time\n","you work through a challenging\n","\n","Scene 8: situation. I thrive in challenges. There was this one time\n","that the computers went down in the middle of\n","the holiday rush. And my coworkers\n","started to panic. So I told them I'll handle it. I briefly closed\n","the store until I was able to get the computers\n","back up and running.\n","\n","Scene 9: While Michael shows\n","initiative in his response,\n","\n","Scene 10: there's a glaring\n","problem with his answer. He only focuses on himself. Employers ask questions\n","about past challenges because they want to see\n","if you're a team player. You can show your\n","leadership capabilities, but also recognize it\n","takes more than just you to get a job done. Moving along. Anne, how will your strengths\n","benefit our company?\n","\n","Scene 11: Great question.\n","\n","Scene 12: Above all, I think my\n","team-focused mentality makes\n","\n","Scene 13: me a great fit for this role. Team dynamic is so-- Oh, wait, sorry, sorry. One final thing I wanted\n","to add to what I said before is that I feel very\n","comfortable leading a team. Stop right there.\n","\n","Scene 14: One of the biggest mistakes\n","you can make during a group interview is cutting\n","another candidate off or trying to speak over them. All group interviews can kick\n","up our competitive nature. Trying to make yourself\n","the loudest in the room will only raise red\n","flags with the employer. Wait until a candidate\n","is done speaking before asking the interviewer\n","if you can chime in. Let's go back to Anne and this\n","time without the interruption.\n","\n","Scene 15: Anne, how will your strengths\n","benefit our company? That's a great question. Above all, I think my\n","team-focused mentality makes\n","\n","Scene 16: me a great fit for this role. Team dynamic is so\n","important to me.\n","\n","Scene 17: And I know that when\n","we're in sync as a unit,\n","\n","Scene 18: we'll have the best\n","outcomes with our customers. I'm also very patient\n","in stressful situations.\n","\n","Scene 19: I love what Chloe said\n","earlier about working through issues with customers. Often, my coworkers\n","will come to me in those situations,\n","especially when they get a little bit\n","heated because I'm\n","\n","Scene 20: able to diffuse them and\n","come up with a solution. That was brilliant.\n","\n","Scene 21: There's so much to\n","like in this answer. Anne made teamwork one of her\n","core strengths, which will be a good sign to any employer. Not only that, but\n","she specifically referenced another candidate's\n","answer in her response and added to it. This shows that she is\n","listening to the room and also moving the\n","conversation forward. Well done. While a group interview might\n","be out of your comfort zone, there's no need to\n","fear heading into it. Be yourself. Speak confidently. Listen to what the other\n","candidates are saying. And don't speak over them. You'll be on your way\n","to that next round of a solo interview in no time. For more interview help, make\n","sure to like and subscribe.\n","\n","Scene 22: Oh, that's funny. I'm going to beep us in. Hello, Charlie. Money.\n","\n"]}],"source":["import os\n","import cv2\n","import numpy as np\n","from segment_anything import sam_model_registry, SamPredictor\n","from youtube_transcript_api import YouTubeTranscriptApi\n","\n","def video_to_frames(video_path, output_dir, frame_rate=0.7):\n","    if not os.path.exists(output_dir):\n","        os.makedirs(output_dir)\n","    cap = cv2.VideoCapture(video_path)\n","    fps = cap.get(cv2.CAP_PROP_FPS)\n","    frame_interval = int(fps / frame_rate)\n","    frame_count = 0\n","    while True:\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","        if frame_count % frame_interval == 0:\n","            cv2.imwrite(os.path.join(output_dir, f'frame_{frame_count:05d}.jpg'), frame)\n","        frame_count += 1\n","    cap.release()\n","    return fps\n","\n","def select_background_points(image, num_points=4):\n","    \"\"\"\n","    Select background points from the edges of the image.\n","    The points will be selected from corners or edges assuming they are likely to be background.\n","    \"\"\"\n","    h, w, _ = image.shape\n","    points = np.array([\n","        [0, 0],  # top-left corner\n","        [0, w - 1],  # top-right corner\n","        [h - 1, 0],  # bottom-left corner\n","        [h - 1, w - 1]  # bottom-right corner\n","    ])\n","    \n","    if num_points > 4:\n","        # Add midpoints of edges as background points if more points are required\n","        points = np.vstack([points, \n","                            [0, w // 2], \n","                            [h // 2, 0], \n","                            [h - 1, w // 2], \n","                            [h // 2, w - 1]])\n","    \n","    return points\n","\n","def compare_histograms(frame1, frame2, threshold=0.4):\n","    \"\"\"Compares histograms of two frames and returns True if they differ significantly.\"\"\"\n","    hist1 = cv2.calcHist([frame1], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])\n","    hist2 = cv2.calcHist([frame2], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])\n","    hist1 = cv2.normalize(hist1, hist1).flatten()\n","    hist2 = cv2.normalize(hist2, hist2).flatten()\n","    diff = cv2.compareHist(hist1, hist2, cv2.HISTCMP_CORREL)\n","    return diff < threshold\n","\n","def detect_scene_changes(frame_dir, fps, threshold=0.15, hist_threshold=0.3):\n","    frames = sorted(os.listdir(frame_dir))\n","    scene_changes = []\n","    prev_mask = None\n","    prev_frame = None\n","\n","    for i, frame_name in enumerate(frames):\n","        frame = cv2.imread(os.path.join(frame_dir, frame_name))\n","        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","        predictor.set_image(frame_rgb)\n","                # Select background points\n","        background_points = select_background_points(frame_rgb)\n","        point_labels = np.zeros(background_points.shape[0], dtype=int)  # Label points as background (0)\n","        \n","        # Obtain masks for the frame focusing on background\n","        masks, _, _ = predictor.predict(point_coords=background_points, \n","                                        point_labels=point_labels, \n","                                        multimask_output=False)\n","        # Compare masks with the previous frame using logical XOR\n","        mask_diff = 0\n","        if prev_mask is not None:\n","            mask_diff = np.logical_xor(masks[0], prev_mask).mean()\n","        \n","        # Compare histograms for color change detection\n","        hist_diff = False\n","        if prev_frame is not None:\n","            hist_diff = compare_histograms(prev_frame, frame, threshold=hist_threshold)\n","        \n","        if mask_diff > threshold or hist_diff:  # Scene change detected\n","            timestamp = int(frame_name.split('_')[1].split('.')[0]) / fps\n","            scene_changes.append(timestamp)\n","        \n","        prev_mask = masks[0]\n","        prev_frame = frame\n","    \n","    return scene_changes\n","\n","def get_transcript(video_id):\n","    try:\n","        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n","        return transcript\n","    except Exception as e:\n","        print(f\"Error retrieving transcript: {e}\")\n","        return []\n","\n","def group_transcripts_by_scenes(transcripts, scene_changes):\n","    grouped_transcripts = []\n","    scene_index = 0\n","    current_group = []\n","\n","    for transcript in transcripts:\n","        start_time = transcript['start']\n","        if scene_index < len(scene_changes) and start_time > scene_changes[scene_index]:\n","            grouped_transcripts.append(' '.join([t['text'] for t in current_group]))\n","            current_group = []\n","            scene_index += 1\n","        current_group.append(transcript)\n","    \n","    if current_group:\n","        grouped_transcripts.append(' '.join([t['text'] for t in current_group]))\n","    \n","    return grouped_transcripts\n","\n","# Path to the video file and directory to save frames\n","video_path = \"/kaggle/input/videoin/How to Ace Your Group Interview _ Mock Job Interview _ Indeed Career Tips.mp4\"\n","output_dir = \"/kaggle/working/freames2\"\n","\n","# Extract frames from the video\n","fps = video_to_frames(video_path, output_dir, frame_rate=0.7)\n","\n","# Initialize the SAM predictor\n","#model = sam_model_registry[\"vit_h\"](checkpoint=\"/content/sam_vit_h_4b8939.pth\")\n","#predictor = SamPredictor(model)\n","\n","# Detect scene changes\n","scene_changes = detect_scene_changes(output_dir, fps, threshold=0.15, hist_threshold=0.3)\n","print(\"Scene changes detected at timestamps (in seconds):\", scene_changes)\n","\n","# Get YouTube transcript\n","video_id = \"eLxA6hPaStw\"\n","transcripts = get_transcript(video_id)\n","\n","# Group transcripts by scene changes\n","grouped_transcripts = group_transcripts_by_scenes(transcripts, scene_changes)\n","for i, text in enumerate(grouped_transcripts):\n","    print(f\"Scene {i + 1}: {text}\\n\")\n"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-08-19T09:47:07.883849Z","iopub.status.busy":"2024-08-19T09:47:07.883274Z","iopub.status.idle":"2024-08-19T09:51:19.646047Z","shell.execute_reply":"2024-08-19T09:51:19.645088Z","shell.execute_reply.started":"2024-08-19T09:47:07.883813Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Scene changes detected at timestamps (in seconds): [8.5085, 12.762749999999999, 48.21483333333333, 69.48608333333333, 82.24883333333332, 102.10199999999999, 109.19241666666666, 126.20941666666666, 143.22641666666667, 163.07958333333332, 181.51466666666664, 182.93275, 184.35083333333333, 191.44125, 201.36783333333332, 225.47525, 226.89333333333332, 229.72949999999997, 232.56566666666666, 242.49224999999998, 249.58266666666665, 252.4188333333333, 258.09116666666665, 296.37941666666666]\n","Scene 1: I hope I'm able to stand\n","out in this interview. What if I say the wrong thing? Wow, they both look\n","really professional. Am I dressed correctly? [MUSIC PLAYING]\n","\n","Scene 2: If a job you've applied to\n","has a lot of other applicants,\n","\n","Scene 3: chances are you might find\n","yourself starting the interview process in a group setting. But don't be alarmed. Group interviews are a\n","great way for employers to get an initial read on\n","you and your work ethic. And above all, they want\n","to see how you communicate and how you work with a team. While they might seem\n","daunting at first, they don't have to be, if you\n","go in with the right mindset. Today, we're going to highlight\n","some best practices for a group interview. With them in mind, securing\n","a one-on-one interview after the group interview\n","should be no problem. [MUSIC PLAYING]\n","\n","Scene 4: Good morning, everyone. I'm Anne. Good morning. Good to meet you all. Michael. Nice to meet you. Chloe. Nice to meet you. Oh, Chloe, Michael. What's great here is\n","that all three candidates are early to the interview. That's a must. But notice how the\n","first two weren't speaking when Anne walked in. Avoid the urge to\n","check your phone\n","\n","Scene 5: or sit silently while waiting\n","for the interviewer to arrive. Like Anne did, start the\n","conversation right away and get to know the\n","other candidates. This will only help during\n","the actual interview.\n","\n","Scene 6: Speaking of, the\n","interviewer has now arrived. So let's get into it. Chloe, how would you\n","describe yourself? Let's see, I've worked\n","in customer service for the last six years, starting\n","in the restaurant industry and then eventually\n","making my way to retail. When asked during a group\n","interview about yourself,\n","\n","Scene 7: don't just recite what can\n","be found on your resume. Share a personal story,\n","something that the interviewer will remember.\n","\n","Scene 8: I am extremely passionate\n","about working with people. I was named employee\n","of the month two times in my current role because\n","of my problem-solving nature in those situations. And I'm excited to hopefully\n","do that soon here as well. That was a great finish.\n","\n","Scene 9: Don't be afraid to brag a\n","little during a group interview when asked about yourself. Be confident, and bring\n","up your accomplishments, especially if it pertains to\n","the job you're applying for. This will help differentiate\n","you from the other candidates. Let's continue. Michael, tell me about a time\n","you work through a challenging\n","\n","Scene 10: situation. I thrive in challenges. There was this one time\n","that the computers went down in the middle of\n","the holiday rush. And my coworkers\n","started to panic. So I told them I'll handle it. I briefly closed\n","the store until I was able to get the computers\n","back up and running. While Michael shows\n","initiative in his response,\n","\n","Scene 11: there's a glaring\n","problem with his answer. He only focuses on himself. Employers ask questions\n","about past challenges because they want to see\n","if you're a team player. You can show your\n","leadership capabilities, but also recognize it\n","takes more than just you to get a job done. Moving along. Anne, how will your strengths\n","benefit our company?\n","\n","Scene 12: Great question.\n","\n","Scene 13: Above all, I think my\n","team-focused mentality makes\n","\n","Scene 14: me a great fit for this role. Team dynamic is so-- Oh, wait, sorry, sorry.\n","\n","Scene 15: One final thing I wanted\n","to add to what I said before is that I feel very\n","comfortable leading a team. Stop right there.\n","\n","Scene 16: One of the biggest mistakes\n","you can make during a group interview is cutting\n","another candidate off or trying to speak over them. All group interviews can kick\n","up our competitive nature. Trying to make yourself\n","the loudest in the room will only raise red\n","flags with the employer. Wait until a candidate\n","is done speaking before asking the interviewer\n","if you can chime in. Let's go back to Anne and this\n","time without the interruption.\n","\n","Scene 17: Anne, how will your strengths\n","benefit our company?\n","\n","Scene 18: That's a great question.\n","\n","Scene 19: Above all, I think my\n","team-focused mentality makes\n","\n","Scene 20: me a great fit for this role. Team dynamic is so\n","important to me. And I know that when\n","we're in sync as a unit, we'll have the best\n","outcomes with our customers. I'm also very patient\n","in stressful situations.\n","\n","Scene 21: I love what Chloe said\n","earlier about working through issues with customers. Often, my coworkers\n","will come to me\n","\n","Scene 22: in those situations,\n","especially when they get a little bit\n","heated because I'm\n","\n","Scene 23: able to diffuse them and\n","come up with a solution. That was brilliant.\n","\n","Scene 24: There's so much to\n","like in this answer. Anne made teamwork one of her\n","core strengths, which will be a good sign to any employer. Not only that, but\n","she specifically referenced another candidate's\n","answer in her response and added to it. This shows that she is\n","listening to the room and also moving the\n","conversation forward. Well done. While a group interview might\n","be out of your comfort zone, there's no need to\n","fear heading into it. Be yourself. Speak confidently. Listen to what the other\n","candidates are saying. And don't speak over them. You'll be on your way\n","to that next round of a solo interview in no time. For more interview help, make\n","sure to like and subscribe.\n","\n","Scene 25: Oh, that's funny. I'm going to beep us in. Hello, Charlie. Money.\n","\n"]}],"source":["import os\n","import cv2\n","import numpy as np\n","from segment_anything import sam_model_registry, SamPredictor\n","from youtube_transcript_api import YouTubeTranscriptApi\n","\n","def video_to_frames(video_path, output_dir, frame_rate=0.7):\n","    if not os.path.exists(output_dir):\n","        os.makedirs(output_dir)\n","    cap = cv2.VideoCapture(video_path)\n","    fps = cap.get(cv2.CAP_PROP_FPS)\n","    frame_interval = int(fps / frame_rate)\n","    frame_count = 0\n","    while True:\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","        if frame_count % frame_interval == 0:\n","            cv2.imwrite(os.path.join(output_dir, f'frame_{frame_count:05d}.jpg'), frame)\n","        frame_count += 1\n","    cap.release()\n","    return fps\n","\n","def select_background_points(image, num_points=4):\n","    \"\"\"\n","    Select background points from the edges of the image.\n","    The points will be selected from corners or edges assuming they are likely to be background.\n","    \"\"\"\n","    h, w, _ = image.shape\n","    points = np.array([\n","        [0, 0],  # top-left corner\n","        [0, w - 1],  # top-right corner\n","        [h - 1, 0],  # bottom-left corner\n","        [h - 1, w - 1]  # bottom-right corner\n","    ])\n","    \n","    if num_points > 4:\n","        # Add midpoints of edges as background points if more points are required\n","        points = np.vstack([points, \n","                            [0, w // 2], \n","                            [h // 2, 0], \n","                            [h - 1, w // 2], \n","                            [h // 2, w - 1]])\n","    \n","    return points\n","\n","def compare_histograms(frame1, frame2, threshold=0.4):\n","    \"\"\"Compares histograms of two frames and returns True if they differ significantly.\"\"\"\n","    hist1 = cv2.calcHist([frame1], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])\n","    hist2 = cv2.calcHist([frame2], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])\n","    hist1 = cv2.normalize(hist1, hist1).flatten()\n","    hist2 = cv2.normalize(hist2, hist2).flatten()\n","    diff = cv2.compareHist(hist1, hist2, cv2.HISTCMP_CORREL)\n","    return diff < threshold\n","\n","def detect_scene_changes(frame_dir, fps, threshold=0.15, hist_threshold=0.3):\n","    frames = sorted(os.listdir(frame_dir))\n","    scene_changes = []\n","    prev_mask = None\n","    prev_frame = None\n","\n","    for i, frame_name in enumerate(frames):\n","        frame = cv2.imread(os.path.join(frame_dir, frame_name))\n","        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","        predictor.set_image(frame_rgb)\n","                # Select background points\n","        background_points = select_background_points(frame_rgb)\n","        point_labels = np.zeros(background_points.shape[0], dtype=int)  # Label points as background (0)\n","        \n","        # Obtain masks for the frame focusing on background\n","        masks, _, _ = predictor.predict(point_coords=background_points, \n","                                        point_labels=point_labels, \n","                                        multimask_output=False)\n","        # Compare masks with the previous frame using logical XOR\n","        mask_diff = 0\n","        if prev_mask is not None:\n","            mask_diff = np.logical_xor(masks[0], prev_mask).mean()\n","        \n","        # Compare histograms for color change detection\n","        hist_diff = False\n","        if prev_frame is not None:\n","            hist_diff = compare_histograms(prev_frame, frame, threshold=hist_threshold)\n","        \n","        if mask_diff > threshold or hist_diff:  # Scene change detected\n","            timestamp = int(frame_name.split('_')[1].split('.')[0]) / fps\n","            scene_changes.append(timestamp)\n","        \n","        prev_mask = masks[0]\n","        prev_frame = frame\n","    \n","    return scene_changes\n","\n","def get_transcript(video_id):\n","    try:\n","        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n","        return transcript\n","    except Exception as e:\n","        print(f\"Error retrieving transcript: {e}\")\n","        return []\n","\n","def group_transcripts_by_scenes(transcripts, scene_changes):\n","    grouped_transcripts = []\n","    scene_index = 0\n","    current_group = []\n","\n","    for transcript in transcripts:\n","        start_time = transcript['start']\n","        if scene_index < len(scene_changes) and start_time > scene_changes[scene_index]:\n","            grouped_transcripts.append(' '.join([t['text'] for t in current_group]))\n","            current_group = []\n","            scene_index += 1\n","        current_group.append(transcript)\n","    \n","    if current_group:\n","        grouped_transcripts.append(' '.join([t['text'] for t in current_group]))\n","    \n","    return grouped_transcripts\n","\n","# Path to the video file and directory to save frames\n","video_path = \"/kaggle/input/videoin/How to Ace Your Group Interview _ Mock Job Interview _ Indeed Career Tips.mp4\"\n","output_dir = \"/kaggle/working/freames3\"\n","\n","# Extract frames from the video\n","fps = video_to_frames(video_path, output_dir, frame_rate=0.7)\n","\n","# Initialize the SAM predictor\n","#model = sam_model_registry[\"vit_h\"](checkpoint=\"/content/sam_vit_h_4b8939.pth\")\n","#predictor = SamPredictor(model)\n","\n","# Detect scene changes\n","scene_changes = detect_scene_changes(output_dir, fps, threshold=0.2, hist_threshold=0.5)\n","print(\"Scene changes detected at timestamps (in seconds):\", scene_changes)\n","\n","# Get YouTube transcript\n","video_id = \"eLxA6hPaStw\"\n","transcripts = get_transcript(video_id)\n","\n","# Group transcripts by scene changes\n","grouped_transcripts = group_transcripts_by_scenes(transcripts, scene_changes)\n","for i, text in enumerate(grouped_transcripts):\n","    print(f\"Scene {i + 1}: {text}\\n\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#This code saves the transcript directly to specified location as txt file\n","\n","import os\n","import cv2\n","import numpy as np\n","from segment_anything import sam_model_registry, SamPredictor\n","from youtube_transcript_api import YouTubeTranscriptApi\n","\n","def video_to_frames(video_path, output_dir, frame_rate=0.7):\n","    if not os.path.exists(output_dir):\n","        os.makedirs(output_dir)\n","    cap = cv2.VideoCapture(video_path)\n","    fps = cap.get(cv2.CAP_PROP_FPS)\n","    frame_interval = int(fps / frame_rate)\n","    frame_count = 0\n","    while True:\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","        if frame_count % frame_interval == 0:\n","            cv2.imwrite(os.path.join(output_dir, f'frame_{frame_count:05d}.jpg'), frame)\n","        frame_count += 1\n","    cap.release()\n","    return fps\n","\n","def select_background_points(image, num_points=4):\n","    h, w, _ = image.shape\n","    points = np.array([\n","        [0, 0],  # top-left corner\n","        [0, w - 1],  # top-right corner\n","        [h - 1, 0],  # bottom-left corner\n","        [h - 1, w - 1]  # bottom-right corner\n","    ])\n","    \n","    if num_points > 4:\n","        points = np.vstack([points, \n","                            [0, w // 2], \n","                            [h // 2, 0], \n","                            [h - 1, w // 2], \n","                            [h // 2, w - 1]])\n","    \n","    return points\n","\n","def compare_histograms(frame1, frame2, threshold=0.4):\n","    hist1 = cv2.calcHist([frame1], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])\n","    hist2 = cv2.calcHist([frame2], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])\n","    hist1 = cv2.normalize(hist1, hist1).flatten()\n","    hist2 = cv2.normalize(hist2, hist2).flatten()\n","    diff = cv2.compareHist(hist1, hist2, cv2.HISTCMP_CORREL)\n","    return diff < threshold\n","\n","def detect_scene_changes(frame_dir, fps, threshold=0.15, hist_threshold=0.3):\n","    frames = sorted(os.listdir(frame_dir))\n","    scene_changes = []\n","    prev_mask = None\n","    prev_frame = None\n","\n","    for i, frame_name in enumerate(frames):\n","        frame = cv2.imread(os.path.join(frame_dir, frame_name))\n","        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","        predictor.set_image(frame_rgb)\n","        \n","        background_points = select_background_points(frame_rgb)\n","        point_labels = np.zeros(background_points.shape[0], dtype=int)  # Label points as background (0)\n","        \n","        masks, _, _ = predictor.predict(point_coords=background_points, \n","                                        point_labels=point_labels, \n","                                        multimask_output=False)\n","        \n","        mask_diff = 0\n","        if prev_mask is not None:\n","            mask_diff = np.logical_xor(masks[0], prev_mask).mean()\n","        \n","        hist_diff = False\n","        if prev_frame is not None:\n","            hist_diff = compare_histograms(prev_frame, frame, threshold=hist_threshold)\n","        \n","        if mask_diff > threshold or hist_diff:  # Scene change detected\n","            timestamp = int(frame_name.split('_')[1].split('.')[0]) / fps\n","            scene_changes.append(timestamp)\n","        \n","        prev_mask = masks[0]\n","        prev_frame = frame\n","    \n","    return scene_changes\n","\n","def get_transcript(video_id):\n","    try:\n","        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n","        return transcript\n","    except Exception as e:\n","        print(f\"Error retrieving transcript: {e}\")\n","        return []\n","\n","def group_transcripts_by_scenes(transcripts, scene_changes):\n","    grouped_transcripts = []\n","    scene_index = 0\n","    current_group = []\n","\n","    for transcript in transcripts:\n","        start_time = transcript['start']\n","        if scene_index < len(scene_changes) and start_time > scene_changes[scene_index]:\n","            grouped_transcripts.append(' '.join([t['text'] for t in current_group]))\n","            current_group = []\n","            scene_index += 1\n","        current_group.append(transcript)\n","    \n","    if current_group:\n","        grouped_transcripts.append(' '.join([t['text'] for t in current_group]))\n","    \n","    return grouped_transcripts\n","\n","def save_transcripts_to_file(grouped_transcripts, output_file):\n","    with open(output_file, 'w', encoding='utf-8') as f:\n","        for i, text in enumerate(grouped_transcripts):\n","            f.write(f\"Scene {i + 1}:\\n{text}\\n\\n\")\n","    print(f\"Grouped transcripts saved to {output_file}\")\n","\n","# Path to the video file and directory to save frames\n","video_path = \"/kaggle/input/videoin/How to Ace Your Group Interview _ Mock Job Interview _ Indeed Career Tips.mp4\"\n","output_dir = \"/kaggle/working/frames2\"\n","\n","# Extract frames from the video\n","fps = video_to_frames(video_path, output_dir, frame_rate=0.7)\n","\n","# Initialize the SAM predictor\n","#model = sam_model_registry[\"vit_h\"](checkpoint=\"/content/sam_vit_h_4b8939.pth\")\n","#predictor = SamPredictor(model)\n","\n","# Detect scene changes\n","scene_changes = detect_scene_changes(output_dir, fps, threshold=0.15, hist_threshold=0.3)\n","print(\"Scene changes detected at timestamps (in seconds):\", scene_changes)\n","\n","# Get YouTube transcript\n","video_id = \"eLxA6hPaStw\"\n","transcripts = get_transcript(video_id)\n","\n","# Group transcripts by scene changes\n","grouped_transcripts = group_transcripts_by_scenes(transcripts, scene_changes)\n","\n","# Save the grouped transcripts to a file\n","output_file = \"/kaggle/working/grouped_transcripts.txt\"\n","save_transcripts_to_file(grouped_transcripts, output_file)\n"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5563394,"sourceId":9201817,"sourceType":"datasetVersion"}],"dockerImageVersionId":30747,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
