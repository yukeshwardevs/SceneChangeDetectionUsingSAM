{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9201817,"sourceType":"datasetVersion","datasetId":5563394}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch torchvision opencv-python segment-anything\n!pip install scikit-image\n! pip install \\\n'git+https://github.com/facebookresearch/segment-anything.git'\n! pip install -q roboflow supervision\n! wget -q \\\n'https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth'","metadata":{"execution":{"iopub.status.busy":"2024-08-19T13:49:41.777065Z","iopub.execute_input":"2024-08-19T13:49:41.777966Z","iopub.status.idle":"2024-08-19T13:50:48.648812Z","shell.execute_reply.started":"2024-08-19T13:49:41.777932Z","shell.execute_reply":"2024-08-19T13:50:48.647587Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.16.2)\nRequirement already satisfied: opencv-python in /opt/conda/lib/python3.10/site-packages (4.10.0.84)\nRequirement already satisfied: segment-anything in /opt/conda/lib/python3.10/site-packages (1.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.5.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.26.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision) (2.32.3)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (9.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2024.7.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nRequirement already satisfied: scikit-image in /opt/conda/lib/python3.10/site-packages (0.22.0)\nRequirement already satisfied: numpy>=1.22 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (1.26.4)\nRequirement already satisfied: scipy>=1.8 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (1.11.4)\nRequirement already satisfied: networkx>=2.8 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (3.2.1)\nRequirement already satisfied: pillow>=9.0.1 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (9.5.0)\nRequirement already satisfied: imageio>=2.27 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (2.33.1)\nRequirement already satisfied: tifffile>=2022.8.12 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (2023.12.9)\nRequirement already satisfied: packaging>=21 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (21.3)\nRequirement already satisfied: lazy_loader>=0.3 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=21->scikit-image) (3.1.1)\nCollecting git+https://github.com/facebookresearch/segment-anything.git\n  Cloning https://github.com/facebookresearch/segment-anything.git to /tmp/pip-req-build-_nz7_9fk\n  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/segment-anything.git /tmp/pip-req-build-_nz7_9fk\n  Resolved https://github.com/facebookresearch/segment-anything.git to commit 6fdee8f2727f4506cfbbe553e23b895e27956588\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\njupyterlab 4.2.3 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.4.1 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom segment_anything import sam_model_registry,SamPredictor\n\nDEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nMODEL_TYPE = \"vit_h\"\n\nsam = sam_model_registry[MODEL_TYPE](checkpoint=\"/kaggle/working/sam_vit_h_4b8939.pth\")\nsam.to(device=DEVICE)\npredictor = SamPredictor(sam)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-19T13:51:25.495830Z","iopub.execute_input":"2024-08-19T13:51:25.496292Z","iopub.status.idle":"2024-08-19T13:51:34.965952Z","shell.execute_reply.started":"2024-08-19T13:51:25.496251Z","shell.execute_reply":"2024-08-19T13:51:34.964932Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"! pip install youtube-transcript-api\nfrom youtube_transcript_api import YouTubeTranscriptApi","metadata":{"execution":{"iopub.status.busy":"2024-08-19T13:51:46.010098Z","iopub.execute_input":"2024-08-19T13:51:46.011011Z","iopub.status.idle":"2024-08-19T13:51:58.813214Z","shell.execute_reply.started":"2024-08-19T13:51:46.010976Z","shell.execute_reply":"2024-08-19T13:51:58.812004Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Collecting youtube-transcript-api\n  Downloading youtube_transcript_api-0.6.2-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from youtube-transcript-api) (2.32.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->youtube-transcript-api) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->youtube-transcript-api) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->youtube-transcript-api) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->youtube-transcript-api) (2024.7.4)\nDownloading youtube_transcript_api-0.6.2-py3-none-any.whl (24 kB)\nInstalling collected packages: youtube-transcript-api\nSuccessfully installed youtube-transcript-api-0.6.2\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nfrom segment_anything import sam_model_registry, SamPredictor\nfrom youtube_transcript_api import YouTubeTranscriptApi\n\ndef video_to_frames(video_path, output_dir, frame_rate=0.7):\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    cap = cv2.VideoCapture(video_path)\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    frame_interval = int(fps / frame_rate)\n    frame_count = 0\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        if frame_count % frame_interval == 0:\n            cv2.imwrite(os.path.join(output_dir, f'frame_{frame_count:05d}.jpg'), frame)\n        frame_count += 1\n    cap.release()\n    return fps\n\ndef select_background_points(image, num_points=4):\n    \"\"\"\n    Select background points from the edges of the image.\n    The points will be selected from corners or edges assuming they are likely to be background.\n    \"\"\"\n    h, w, _ = image.shape\n    points = np.array([\n        [0, 0],  # top-left corner\n        [0, w - 1],  # top-right corner\n        [h - 1, 0],  # bottom-left corner\n        [h - 1, w - 1]  # bottom-right corner\n    ])\n    \n    if num_points > 4:\n        # Add midpoints of edges as background points if more points are required\n        points = np.vstack([points, \n                            [0, w // 2], \n                            [h // 2, 0], \n                            [h - 1, w // 2], \n                            [h // 2, w - 1]])\n    \n    return points\n\ndef compare_histograms(frame1, frame2, threshold=0.4):\n    \"\"\"Compares histograms of two frames and returns True if they differ significantly.\"\"\"\n    hist1 = cv2.calcHist([frame1], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])\n    hist2 = cv2.calcHist([frame2], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])\n    hist1 = cv2.normalize(hist1, hist1).flatten()\n    hist2 = cv2.normalize(hist2, hist2).flatten()\n    diff = cv2.compareHist(hist1, hist2, cv2.HISTCMP_CORREL)\n    return diff < threshold\n\ndef detect_scene_changes(frame_dir, fps, threshold=0.15, hist_threshold=0.3):\n    frames = sorted(os.listdir(frame_dir))\n    scene_changes = []\n    prev_mask = None\n    prev_frame = None\n\n    for i, frame_name in enumerate(frames):\n        frame = cv2.imread(os.path.join(frame_dir, frame_name))\n        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        predictor.set_image(frame_rgb)\n                # Select background points\n        background_points = select_background_points(frame_rgb)\n        point_labels = np.zeros(background_points.shape[0], dtype=int)  # Label points as background (0)\n        \n        # Obtain masks for the frame focusing on background\n        masks, _, _ = predictor.predict(point_coords=background_points, \n                                        point_labels=point_labels, \n                                        multimask_output=False)\n        # Compare masks with the previous frame using logical XOR\n        mask_diff = 0\n        if prev_mask is not None:\n            mask_diff = np.logical_xor(masks[0], prev_mask).mean()\n        \n        # Compare histograms for color change detection\n        hist_diff = False\n        if prev_frame is not None:\n            hist_diff = compare_histograms(prev_frame, frame, threshold=hist_threshold)\n        \n        if mask_diff > threshold or hist_diff:  # Scene change detected\n            timestamp = int(frame_name.split('_')[1].split('.')[0]) / fps\n            scene_changes.append(timestamp)\n        \n        prev_mask = masks[0]\n        prev_frame = frame\n    \n    return scene_changes\n\ndef get_transcript(video_id):\n    try:\n        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n        return transcript\n    except Exception as e:\n        print(f\"Error retrieving transcript: {e}\")\n        return []\n\ndef group_transcripts_by_scenes(transcripts, scene_changes):\n    grouped_transcripts = []\n    scene_index = 0\n    current_group = []\n\n    for transcript in transcripts:\n        start_time = transcript['start']\n        if scene_index < len(scene_changes) and start_time > scene_changes[scene_index]:\n            grouped_transcripts.append(' '.join([t['text'] for t in current_group]))\n            current_group = []\n            scene_index += 1\n        current_group.append(transcript)\n    \n    if current_group:\n        grouped_transcripts.append(' '.join([t['text'] for t in current_group]))\n    \n    return grouped_transcripts\n\n# Path to the video file and directory to save frames\nvideo_path = \"/kaggle/input/videoin/How to Ace Your Group Interview _ Mock Job Interview _ Indeed Career Tips.mp4\"\noutput_dir = \"/kaggle/working/freames2\"\n\n# Extract frames from the video\nfps = video_to_frames(video_path, output_dir, frame_rate=0.7)\n\n# Initialize the SAM predictor\n#model = sam_model_registry[\"vit_h\"](checkpoint=\"/content/sam_vit_h_4b8939.pth\")\n#predictor = SamPredictor(model)\n\n# Detect scene changes\nscene_changes = detect_scene_changes(output_dir, fps, threshold=0.15, hist_threshold=0.3)\nprint(\"Scene changes detected at timestamps (in seconds):\", scene_changes)\n\n# Get YouTube transcript\nvideo_id = \"eLxA6hPaStw\"\ntranscripts = get_transcript(video_id)\n\n# Group transcripts by scene changes\ngrouped_transcripts = group_transcripts_by_scenes(transcripts, scene_changes)\nfor i, text in enumerate(grouped_transcripts):\n    print(f\"Scene {i + 1}: {text}\\n\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-19T09:41:44.957981Z","iopub.execute_input":"2024-08-19T09:41:44.958694Z","iopub.status.idle":"2024-08-19T09:45:57.043226Z","shell.execute_reply.started":"2024-08-19T09:41:44.958647Z","shell.execute_reply":"2024-08-19T09:45:57.042284Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Scene changes detected at timestamps (in seconds): [12.762749999999999, 48.21483333333333, 69.48608333333333, 82.24883333333332, 102.10199999999999, 126.20941666666666, 143.22641666666667, 161.6615, 163.07958333333332, 181.51466666666664, 182.93275, 184.35083333333333, 201.36783333333332, 225.47525, 232.56566666666666, 236.81991666666664, 238.23799999999997, 242.49224999999998, 252.4188333333333, 258.09116666666665, 296.37941666666666]\nScene 1: I hope I'm able to stand\nout in this interview. What if I say the wrong thing? Wow, they both look\nreally professional. Am I dressed correctly? [MUSIC PLAYING] If a job you've applied to\nhas a lot of other applicants,\n\nScene 2: chances are you might find\nyourself starting the interview process in a group setting. But don't be alarmed. Group interviews are a\ngreat way for employers to get an initial read on\nyou and your work ethic. And above all, they want\nto see how you communicate and how you work with a team. While they might seem\ndaunting at first, they don't have to be, if you\ngo in with the right mindset. Today, we're going to highlight\nsome best practices for a group interview. With them in mind, securing\na one-on-one interview after the group interview\nshould be no problem. [MUSIC PLAYING]\n\nScene 3: Good morning, everyone. I'm Anne. Good morning. Good to meet you all. Michael. Nice to meet you. Chloe. Nice to meet you. Oh, Chloe, Michael. What's great here is\nthat all three candidates are early to the interview. That's a must. But notice how the\nfirst two weren't speaking when Anne walked in. Avoid the urge to\ncheck your phone\n\nScene 4: or sit silently while waiting\nfor the interviewer to arrive. Like Anne did, start the\nconversation right away and get to know the\nother candidates. This will only help during\nthe actual interview.\n\nScene 5: Speaking of, the\ninterviewer has now arrived. So let's get into it. Chloe, how would you\ndescribe yourself? Let's see, I've worked\nin customer service for the last six years, starting\nin the restaurant industry and then eventually\nmaking my way to retail. When asked during a group\ninterview about yourself,\n\nScene 6: don't just recite what can\nbe found on your resume. Share a personal story,\nsomething that the interviewer will remember. I am extremely passionate\nabout working with people. I was named employee\nof the month two times in my current role because\nof my problem-solving nature in those situations. And I'm excited to hopefully\ndo that soon here as well. That was a great finish.\n\nScene 7: Don't be afraid to brag a\nlittle during a group interview when asked about yourself. Be confident, and bring\nup your accomplishments, especially if it pertains to\nthe job you're applying for. This will help differentiate\nyou from the other candidates. Let's continue. Michael, tell me about a time\nyou work through a challenging\n\nScene 8: situation. I thrive in challenges. There was this one time\nthat the computers went down in the middle of\nthe holiday rush. And my coworkers\nstarted to panic. So I told them I'll handle it. I briefly closed\nthe store until I was able to get the computers\nback up and running.\n\nScene 9: While Michael shows\ninitiative in his response,\n\nScene 10: there's a glaring\nproblem with his answer. He only focuses on himself. Employers ask questions\nabout past challenges because they want to see\nif you're a team player. You can show your\nleadership capabilities, but also recognize it\ntakes more than just you to get a job done. Moving along. Anne, how will your strengths\nbenefit our company?\n\nScene 11: Great question.\n\nScene 12: Above all, I think my\nteam-focused mentality makes\n\nScene 13: me a great fit for this role. Team dynamic is so-- Oh, wait, sorry, sorry. One final thing I wanted\nto add to what I said before is that I feel very\ncomfortable leading a team. Stop right there.\n\nScene 14: One of the biggest mistakes\nyou can make during a group interview is cutting\nanother candidate off or trying to speak over them. All group interviews can kick\nup our competitive nature. Trying to make yourself\nthe loudest in the room will only raise red\nflags with the employer. Wait until a candidate\nis done speaking before asking the interviewer\nif you can chime in. Let's go back to Anne and this\ntime without the interruption.\n\nScene 15: Anne, how will your strengths\nbenefit our company? That's a great question. Above all, I think my\nteam-focused mentality makes\n\nScene 16: me a great fit for this role. Team dynamic is so\nimportant to me.\n\nScene 17: And I know that when\nwe're in sync as a unit,\n\nScene 18: we'll have the best\noutcomes with our customers. I'm also very patient\nin stressful situations.\n\nScene 19: I love what Chloe said\nearlier about working through issues with customers. Often, my coworkers\nwill come to me in those situations,\nespecially when they get a little bit\nheated because I'm\n\nScene 20: able to diffuse them and\ncome up with a solution. That was brilliant.\n\nScene 21: There's so much to\nlike in this answer. Anne made teamwork one of her\ncore strengths, which will be a good sign to any employer. Not only that, but\nshe specifically referenced another candidate's\nanswer in her response and added to it. This shows that she is\nlistening to the room and also moving the\nconversation forward. Well done. While a group interview might\nbe out of your comfort zone, there's no need to\nfear heading into it. Be yourself. Speak confidently. Listen to what the other\ncandidates are saying. And don't speak over them. You'll be on your way\nto that next round of a solo interview in no time. For more interview help, make\nsure to like and subscribe.\n\nScene 22: Oh, that's funny. I'm going to beep us in. Hello, Charlie. Money.\n\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nfrom segment_anything import sam_model_registry, SamPredictor\nfrom youtube_transcript_api import YouTubeTranscriptApi\n\ndef video_to_frames(video_path, output_dir, frame_rate=0.7):\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    cap = cv2.VideoCapture(video_path)\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    frame_interval = int(fps / frame_rate)\n    frame_count = 0\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        if frame_count % frame_interval == 0:\n            cv2.imwrite(os.path.join(output_dir, f'frame_{frame_count:05d}.jpg'), frame)\n        frame_count += 1\n    cap.release()\n    return fps\n\ndef select_background_points(image, num_points=4):\n    \"\"\"\n    Select background points from the edges of the image.\n    The points will be selected from corners or edges assuming they are likely to be background.\n    \"\"\"\n    h, w, _ = image.shape\n    points = np.array([\n        [0, 0],  # top-left corner\n        [0, w - 1],  # top-right corner\n        [h - 1, 0],  # bottom-left corner\n        [h - 1, w - 1]  # bottom-right corner\n    ])\n    \n    if num_points > 4:\n        # Add midpoints of edges as background points if more points are required\n        points = np.vstack([points, \n                            [0, w // 2], \n                            [h // 2, 0], \n                            [h - 1, w // 2], \n                            [h // 2, w - 1]])\n    \n    return points\n\ndef compare_histograms(frame1, frame2, threshold=0.4):\n    \"\"\"Compares histograms of two frames and returns True if they differ significantly.\"\"\"\n    hist1 = cv2.calcHist([frame1], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])\n    hist2 = cv2.calcHist([frame2], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])\n    hist1 = cv2.normalize(hist1, hist1).flatten()\n    hist2 = cv2.normalize(hist2, hist2).flatten()\n    diff = cv2.compareHist(hist1, hist2, cv2.HISTCMP_CORREL)\n    return diff < threshold\n\ndef detect_scene_changes(frame_dir, fps, threshold=0.15, hist_threshold=0.3):\n    frames = sorted(os.listdir(frame_dir))\n    scene_changes = []\n    prev_mask = None\n    prev_frame = None\n\n    for i, frame_name in enumerate(frames):\n        frame = cv2.imread(os.path.join(frame_dir, frame_name))\n        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        predictor.set_image(frame_rgb)\n                # Select background points\n        background_points = select_background_points(frame_rgb)\n        point_labels = np.zeros(background_points.shape[0], dtype=int)  # Label points as background (0)\n        \n        # Obtain masks for the frame focusing on background\n        masks, _, _ = predictor.predict(point_coords=background_points, \n                                        point_labels=point_labels, \n                                        multimask_output=False)\n        # Compare masks with the previous frame using logical XOR\n        mask_diff = 0\n        if prev_mask is not None:\n            mask_diff = np.logical_xor(masks[0], prev_mask).mean()\n        \n        # Compare histograms for color change detection\n        hist_diff = False\n        if prev_frame is not None:\n            hist_diff = compare_histograms(prev_frame, frame, threshold=hist_threshold)\n        \n        if mask_diff > threshold or hist_diff:  # Scene change detected\n            timestamp = int(frame_name.split('_')[1].split('.')[0]) / fps\n            scene_changes.append(timestamp)\n        \n        prev_mask = masks[0]\n        prev_frame = frame\n    \n    return scene_changes\n\ndef get_transcript(video_id):\n    try:\n        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n        return transcript\n    except Exception as e:\n        print(f\"Error retrieving transcript: {e}\")\n        return []\n\ndef group_transcripts_by_scenes(transcripts, scene_changes):\n    grouped_transcripts = []\n    scene_index = 0\n    current_group = []\n\n    for transcript in transcripts:\n        start_time = transcript['start']\n        if scene_index < len(scene_changes) and start_time > scene_changes[scene_index]:\n            grouped_transcripts.append(' '.join([t['text'] for t in current_group]))\n            current_group = []\n            scene_index += 1\n        current_group.append(transcript)\n    \n    if current_group:\n        grouped_transcripts.append(' '.join([t['text'] for t in current_group]))\n    \n    return grouped_transcripts\n\n# Path to the video file and directory to save frames\nvideo_path = \"/kaggle/input/videoin/How to Ace Your Group Interview _ Mock Job Interview _ Indeed Career Tips.mp4\"\noutput_dir = \"/kaggle/working/freames3\"\n\n# Extract frames from the video\nfps = video_to_frames(video_path, output_dir, frame_rate=0.7)\n\n# Initialize the SAM predictor\n#model = sam_model_registry[\"vit_h\"](checkpoint=\"/content/sam_vit_h_4b8939.pth\")\n#predictor = SamPredictor(model)\n\n# Detect scene changes\nscene_changes = detect_scene_changes(output_dir, fps, threshold=0.2, hist_threshold=0.5)\nprint(\"Scene changes detected at timestamps (in seconds):\", scene_changes)\n\n# Get YouTube transcript\nvideo_id = \"eLxA6hPaStw\"\ntranscripts = get_transcript(video_id)\n\n# Group transcripts by scene changes\ngrouped_transcripts = group_transcripts_by_scenes(transcripts, scene_changes)\nfor i, text in enumerate(grouped_transcripts):\n    print(f\"Scene {i + 1}: {text}\\n\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-19T09:47:07.883274Z","iopub.execute_input":"2024-08-19T09:47:07.883849Z","iopub.status.idle":"2024-08-19T09:51:19.646047Z","shell.execute_reply.started":"2024-08-19T09:47:07.883813Z","shell.execute_reply":"2024-08-19T09:51:19.645088Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Scene changes detected at timestamps (in seconds): [8.5085, 12.762749999999999, 48.21483333333333, 69.48608333333333, 82.24883333333332, 102.10199999999999, 109.19241666666666, 126.20941666666666, 143.22641666666667, 163.07958333333332, 181.51466666666664, 182.93275, 184.35083333333333, 191.44125, 201.36783333333332, 225.47525, 226.89333333333332, 229.72949999999997, 232.56566666666666, 242.49224999999998, 249.58266666666665, 252.4188333333333, 258.09116666666665, 296.37941666666666]\nScene 1: I hope I'm able to stand\nout in this interview. What if I say the wrong thing? Wow, they both look\nreally professional. Am I dressed correctly? [MUSIC PLAYING]\n\nScene 2: If a job you've applied to\nhas a lot of other applicants,\n\nScene 3: chances are you might find\nyourself starting the interview process in a group setting. But don't be alarmed. Group interviews are a\ngreat way for employers to get an initial read on\nyou and your work ethic. And above all, they want\nto see how you communicate and how you work with a team. While they might seem\ndaunting at first, they don't have to be, if you\ngo in with the right mindset. Today, we're going to highlight\nsome best practices for a group interview. With them in mind, securing\na one-on-one interview after the group interview\nshould be no problem. [MUSIC PLAYING]\n\nScene 4: Good morning, everyone. I'm Anne. Good morning. Good to meet you all. Michael. Nice to meet you. Chloe. Nice to meet you. Oh, Chloe, Michael. What's great here is\nthat all three candidates are early to the interview. That's a must. But notice how the\nfirst two weren't speaking when Anne walked in. Avoid the urge to\ncheck your phone\n\nScene 5: or sit silently while waiting\nfor the interviewer to arrive. Like Anne did, start the\nconversation right away and get to know the\nother candidates. This will only help during\nthe actual interview.\n\nScene 6: Speaking of, the\ninterviewer has now arrived. So let's get into it. Chloe, how would you\ndescribe yourself? Let's see, I've worked\nin customer service for the last six years, starting\nin the restaurant industry and then eventually\nmaking my way to retail. When asked during a group\ninterview about yourself,\n\nScene 7: don't just recite what can\nbe found on your resume. Share a personal story,\nsomething that the interviewer will remember.\n\nScene 8: I am extremely passionate\nabout working with people. I was named employee\nof the month two times in my current role because\nof my problem-solving nature in those situations. And I'm excited to hopefully\ndo that soon here as well. That was a great finish.\n\nScene 9: Don't be afraid to brag a\nlittle during a group interview when asked about yourself. Be confident, and bring\nup your accomplishments, especially if it pertains to\nthe job you're applying for. This will help differentiate\nyou from the other candidates. Let's continue. Michael, tell me about a time\nyou work through a challenging\n\nScene 10: situation. I thrive in challenges. There was this one time\nthat the computers went down in the middle of\nthe holiday rush. And my coworkers\nstarted to panic. So I told them I'll handle it. I briefly closed\nthe store until I was able to get the computers\nback up and running. While Michael shows\ninitiative in his response,\n\nScene 11: there's a glaring\nproblem with his answer. He only focuses on himself. Employers ask questions\nabout past challenges because they want to see\nif you're a team player. You can show your\nleadership capabilities, but also recognize it\ntakes more than just you to get a job done. Moving along. Anne, how will your strengths\nbenefit our company?\n\nScene 12: Great question.\n\nScene 13: Above all, I think my\nteam-focused mentality makes\n\nScene 14: me a great fit for this role. Team dynamic is so-- Oh, wait, sorry, sorry.\n\nScene 15: One final thing I wanted\nto add to what I said before is that I feel very\ncomfortable leading a team. Stop right there.\n\nScene 16: One of the biggest mistakes\nyou can make during a group interview is cutting\nanother candidate off or trying to speak over them. All group interviews can kick\nup our competitive nature. Trying to make yourself\nthe loudest in the room will only raise red\nflags with the employer. Wait until a candidate\nis done speaking before asking the interviewer\nif you can chime in. Let's go back to Anne and this\ntime without the interruption.\n\nScene 17: Anne, how will your strengths\nbenefit our company?\n\nScene 18: That's a great question.\n\nScene 19: Above all, I think my\nteam-focused mentality makes\n\nScene 20: me a great fit for this role. Team dynamic is so\nimportant to me. And I know that when\nwe're in sync as a unit, we'll have the best\noutcomes with our customers. I'm also very patient\nin stressful situations.\n\nScene 21: I love what Chloe said\nearlier about working through issues with customers. Often, my coworkers\nwill come to me\n\nScene 22: in those situations,\nespecially when they get a little bit\nheated because I'm\n\nScene 23: able to diffuse them and\ncome up with a solution. That was brilliant.\n\nScene 24: There's so much to\nlike in this answer. Anne made teamwork one of her\ncore strengths, which will be a good sign to any employer. Not only that, but\nshe specifically referenced another candidate's\nanswer in her response and added to it. This shows that she is\nlistening to the room and also moving the\nconversation forward. Well done. While a group interview might\nbe out of your comfort zone, there's no need to\nfear heading into it. Be yourself. Speak confidently. Listen to what the other\ncandidates are saying. And don't speak over them. You'll be on your way\nto that next round of a solo interview in no time. For more interview help, make\nsure to like and subscribe.\n\nScene 25: Oh, that's funny. I'm going to beep us in. Hello, Charlie. Money.\n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}